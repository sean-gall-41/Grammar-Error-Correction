{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F6uK1ehNF9pm",
        "outputId": "639f952d-61e3-469a-bed6-48538814d7cb"
      },
      "outputs": [],
      "source": [
        "!pip install gdown --no-cache-dir -U"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5d_kNrSGDZ8"
      },
      "source": [
        "# Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HPHyhqQ29SZH"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fZ2NTiDl-Kb4"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from timeit import default_timer as timer\n",
        "\n",
        "import pathlib as pl\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPvXtF9kAPeP"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vphVs_6J-j_6",
        "outputId": "a5614fc7-eee1-42a9-93c9-e9e93a72d465"
      },
      "outputs": [],
      "source": [
        "!gdown \"18d7-qbKjt2uS1ORdvVIr8LBrTqdZYaTI\"\n",
        "!tar xvjf \"/content/C4_200M.hdf5-00001.3-of-00010.tar.bz2\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kXVftQJS-Mot"
      },
      "outputs": [],
      "source": [
        "import h5py\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class Hdf5Dataset(Dataset):\n",
        "    \"\"\"Custom Dataset for loading entries from HDF5 databases\"\"\"\n",
        "\n",
        "    def __init__(self, h5_path, transform=None, num_entries=None):\n",
        "        self.h5f = h5py.File(h5_path, \"r\")\n",
        "        if num_entries:\n",
        "            self.num_entries = num_entries\n",
        "        else:\n",
        "            self.num_entries = self.h5f[\"labels\"].shape[0]\n",
        "        self.transform = transform\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        if index > self.num_entries:\n",
        "            raise StopIteration\n",
        "        input = self.h5f[\"input\"][index].decode(\"utf-8\")\n",
        "        label = self.h5f[\"labels\"][index].decode(\"utf-8\")\n",
        "        if self.transform is not None:\n",
        "            features = self.transform(input)\n",
        "        return input, label\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_entries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GVNak5A4ATbV"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wnkb5Uhv-Pod"
      },
      "outputs": [],
      "source": [
        "from typing import Iterable, List\n",
        "from tqdm import tqdm\n",
        "import pathlib as pl\n",
        "from torchtext.data import get_tokenizer\n",
        "\n",
        "# helper function to yield list of tokens\n",
        "def yield_tokens(data_iter: Iterable, index: int) -> List[str]:\n",
        "    language_index = {SRC_LANGUAGE: 0, TGT_LANGUAGE: 1}\n",
        "    for data_sample in tqdm(data_iter):\n",
        "        if data_sample[index] and isinstance(data_sample[index], str):\n",
        "            yield token_transform(data_sample[index])\n",
        "\n",
        "SRC_LANGUAGE = \"incorrect\"\n",
        "TGT_LANGUAGE = \"correct\"\n",
        "\n",
        "MAX_LENGTH = 512\n",
        "VOCAB_SIZE = 20000\n",
        "N_SAMPLES = 20000\n",
        "\n",
        "# Place-holders\n",
        "token_transform = get_tokenizer(\"basic_english\")\n",
        "vocab_transform = None\n",
        "\n",
        "folder = \"./data\"\n",
        "train_filename = \"C4_200M.hdf5-00000-of-00010\"\n",
        "valid_filename = \"C4_200M.hdf5-00001-of-00010\"\n",
        "\n",
        "embedding_path = \"./glove.42B.300d.txt\"\n",
        "\n",
        "checkpoint_folder = \"./checkpoints\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kX06GSI9GcKX",
        "outputId": "5047c692-dfef-4e3b-af1f-3964043d5f6e"
      },
      "outputs": [],
      "source": [
        "import gdown\n",
        "\n",
        "gdown.download_folder(\n",
        "    \"https://drive.google.com/drive/folders/1FQ_jm765fgwcD5lLtjl6ef9k532hdADR\",\n",
        "    quiet=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JPeUeY0dA6YP"
      },
      "outputs": [],
      "source": [
        "# Define special symbols and indices\n",
        "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
        "\n",
        "# Make sure the tokens are in order of their indices to properly insert them in vocab\n",
        "special_symbols = [\"<UNK>\", \"<PAD>\", \"<BOS>\", \"<EOS>\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YDDjmMopOBqz"
      },
      "outputs": [],
      "source": [
        "from torchtext.vocab import GloVe, vocab\n",
        "\n",
        "def pretrained_embs(name: str, dim: str, max_vectors: int = None):\n",
        "    glove_vectors = GloVe(name=name, dim=dim, max_vectors=max_vectors)\n",
        "    glove_vocab = vocab(glove_vectors.stoi)\n",
        "    pretrained_embeddings = glove_vectors.vectors\n",
        "    glove_vocab.insert_token(\"<UNK>\", UNK_IDX)\n",
        "    pretrained_embeddings = torch.cat(\n",
        "        (torch.mean(pretrained_embeddings, dim=0, keepdims=True), pretrained_embeddings)\n",
        "    )\n",
        "    glove_vocab.insert_token(\"<PAD>\", PAD_IDX)\n",
        "    pretrained_embeddings = torch.cat(\n",
        "        (torch.zeros(1, pretrained_embeddings.shape[1]), pretrained_embeddings)\n",
        "    )\n",
        "    glove_vocab.insert_token(\"<BOS>\", PAD_IDX)\n",
        "    pretrained_embeddings = torch.cat(\n",
        "        (torch.rand(1, pretrained_embeddings.shape[1]), pretrained_embeddings)\n",
        "    )\n",
        "    glove_vocab.insert_token(\"<EOS>\", PAD_IDX)\n",
        "    pretrained_embeddings = torch.cat(\n",
        "        (torch.rand(1, pretrained_embeddings.shape[1]), pretrained_embeddings)\n",
        "    )\n",
        "    glove_vocab.set_default_index(UNK_IDX)\n",
        "    return glove_vocab, pretrained_embeddings\n",
        "\n",
        "vocab, embeddings = pretrained_embs(\"42B\", \"300\", 20000)\n",
        "\n",
        "torch.save(embeddings, \"glove.42B.300d.20K.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2gr7PgrPOrGj"
      },
      "outputs": [],
      "source": [
        "# Load vocabulary and pretrained embeddings\n",
        "\n",
        "vocab_transform = torch.load(\"vocab/vocab_20K.pth\")\n",
        "embeddings = torch.load(\"glove.42B.300d.20K.pth\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDx_3gmfxUuN"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4An4xCwA-8k"
      },
      "source": [
        "## Collation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gr8ARUKFA8zO"
      },
      "outputs": [],
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "# helper function to club together sequential operations\n",
        "def sequential_transforms(*transforms):\n",
        "    def func(txt_input):\n",
        "        for transform in transforms:\n",
        "            txt_input = transform(txt_input)\n",
        "        return txt_input\n",
        "    return func\n",
        "\n",
        "\n",
        "# function to add BOS/EOS and create tensor for input sequence indices\n",
        "def tensor_transform(token_ids: List[int]):\n",
        "    return torch.cat(\n",
        "        (torch.tensor([BOS_IDX]), torch.tensor(token_ids), torch.tensor([EOS_IDX]))\n",
        "    )\n",
        "\n",
        "# src and tgt language text transforms to convert raw strings into tensors indices\n",
        "text_transform = sequential_transforms(\n",
        "    token_transform, vocab_transform, tensor_transform\n",
        ")  # Add BOS/EOS and create tensor\n",
        "\n",
        "\n",
        "# function to collate data samples into batch tesors\n",
        "def collate_fn(batch):\n",
        "    src_batch, tgt_batch = [], []\n",
        "    for src_sample, tgt_sample in batch:\n",
        "        src_batch.append(text_transform(src_sample.rstrip(\"\\n\")))\n",
        "        tgt_batch.append(text_transform(tgt_sample.rstrip(\"\\n\")))\n",
        "\n",
        "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX)\n",
        "    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX)\n",
        "    return src_batch, tgt_batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VZPC2VdeBAL5",
        "outputId": "85b11680-5434-4c13-e03f-d61e184dcd3f"
      },
      "outputs": [],
      "source": [
        "text = \"data mining is awesome!\"\n",
        "tokenized_input = token_transform(text)\n",
        "print(\"tokenized input:\\n\", tokenized_input)\n",
        "\n",
        "encoded_input = vocab_transform(tokenized_input)\n",
        "print(\"encoded input:\\n\", encoded_input)\n",
        "\n",
        "print(\"transformed input:\\n\", text_transform(text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NbpCULVLBFby"
      },
      "source": [
        "## Unknown words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iN82KeuGBHdb",
        "outputId": "6ae46bec-0cf5-49f6-8ac0-6b4dd6c77e23"
      },
      "outputs": [],
      "source": [
        "text = \"dataminingisawesome!\"\n",
        "tokenized_input = token_transform(text)\n",
        "print(tokenized_input)\n",
        "\n",
        "encoded_input = vocab_transform(tokenized_input)\n",
        "print(encoded_input)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jswthNvABLAT"
      },
      "source": [
        "RNN Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4rarHnwuBL0r"
      },
      "outputs": [],
      "source": [
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(DEVICE)\n",
        "\n",
        "teacher_forcing_ratio = 0.5\n",
        "torch.manual_seed(0)\n",
        "\n",
        "EMB_SIZE = 300\n",
        "HIDDEN_SIZE = 512\n",
        "BATCH_SIZE = 16\n",
        "NUM_ENCODER_LAYERS = 1\n",
        "NUM_DECODER_LAYERS = 1\n",
        "\n",
        "learning_rate = 0.001"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1RLZ_q2iBQIV"
      },
      "outputs": [],
      "source": [
        "def generate_square_subsequent_mask(sz):\n",
        "    mask = (torch.triu(torch.ones((sz, sz), device=DEVICE)) == 1).transpose(0, 1)\n",
        "    mask = (\n",
        "        mask.float()\n",
        "        .masked_fill(mask == 0, float(\"-inf\"))\n",
        "        .masked_fill(mask == 1, float(0.0))\n",
        "    )\n",
        "    return mask\n",
        "\n",
        "\n",
        "def create_mask(src):\n",
        "    src_seq_len = src.shape[0]\n",
        "\n",
        "    src_mask = torch.zeros((src_seq_len, src_seq_len), device=DEVICE).type(torch.bool)\n",
        "\n",
        "    src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n",
        "    return src_mask, src_padding_mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HZI1idP-BUe0"
      },
      "outputs": [],
      "source": [
        "def train(model, iterator, optimizer, criterion, clip):\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    epoch_loss = 0\n",
        "\n",
        "    for src, trg in tqdm(iterator):\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        src = src.to(DEVICE)\n",
        "        trg = trg.to(DEVICE)\n",
        "\n",
        "        output = model(src, trg)\n",
        "\n",
        "        #trg = [trg len, batch size]\n",
        "        #output = [trg len, batch size, output dim]\n",
        "\n",
        "        output_dim = output.shape[-1]\n",
        "        output = output[1:].view(-1, output_dim)\n",
        "        trg = trg[1:].view(-1)\n",
        "\n",
        "        #trg = [(trg len - 1) * batch size]\n",
        "        #output = [(trg len - 1) * batch size, output dim]\n",
        "\n",
        "        loss = criterion(output, trg)\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(iterator)\n",
        "\n",
        "def evaluate(model, iterator, criterion):\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    epoch_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for src, trg in tqdm(iterator):\n",
        "\n",
        "            src = src.to(DEVICE)\n",
        "            trg = trg.to(DEVICE)\n",
        "\n",
        "            output = model(src, trg, 0) #turn off teacher forcing\n",
        "\n",
        "            #trg = [trg len, batch size]\n",
        "            #output = [trg len, batch size, output dim]\n",
        "\n",
        "            output_dim = output.shape[-1]\n",
        "            output = output[1:].view(-1, output_dim)\n",
        "            trg = trg[1:].view(-1)\n",
        "\n",
        "            #trg = [(trg len - 1) * batch size]\n",
        "            #output = [(trg len - 1) * batch size]\n",
        "\n",
        "            loss = criterion(output, trg)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(iterator)\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QN6QA4x_W2j9"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from random import random\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "MAX_LENGTH = 512\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, hid_dim, dropout, embedding_weights=None):\n",
        "        super().__init__()\n",
        "\n",
        "        self.hid_dim = hid_dim\n",
        "\n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim, padding_idx=1)\n",
        "\n",
        "        if embedding_weights is not None:\n",
        "            self.embedding.weight = torch.nn.Parameter(\n",
        "                torch.from_numpy(embedding_weights)\n",
        "            )\n",
        "\n",
        "        self.rnn = nn.GRU(emb_dim, hid_dim)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, src):\n",
        "\n",
        "        embedded = self.dropout(self.embedding(src).float())\n",
        "\n",
        "        # embedded = [src len, batch size, emb dim]\n",
        "\n",
        "        outputs, hidden = self.rnn(embedded)  # no cell state!\n",
        "\n",
        "        # outputs = [src len, batch size, hid dim * n directions]\n",
        "        # hidden = [n layers * n directions, batch size, hid dim]\n",
        "        # outputs are always from the top hidden layer\n",
        "\n",
        "        return hidden\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, emb_dim, hid_dim, dropout, embedding_weights=None):\n",
        "        super().__init__()\n",
        "\n",
        "        self.hid_dim = hid_dim\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim, padding_idx=1)\n",
        "\n",
        "        if embedding_weights is not None:\n",
        "            self.embedding.weight = torch.nn.Parameter(\n",
        "                torch.from_numpy(embedding_weights)\n",
        "            )\n",
        "\n",
        "        self.rnn = nn.GRU(emb_dim + hid_dim, hid_dim)\n",
        "\n",
        "        # self.fc_out = nn.Linear(hid_dim, output_dim)\n",
        "        self.fc_out = nn.Linear(emb_dim + hid_dim * 2, output_dim)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, input, hidden, context):\n",
        "        # input = [batch size]\n",
        "        # hidden = [n layers * n directions, batch size, hid dim]\n",
        "        # context = [n layers * n directions, batch size, hid dim]\n",
        "\n",
        "        # n layers and n directions in the decoder will both always be 1, therefore:\n",
        "        # hidden = [1, batch size, hid dim]\n",
        "        # context = [1, batch size, hid dim]\n",
        "\n",
        "        input = input.unsqueeze(0)\n",
        "\n",
        "        # input = [1, batch size]\n",
        "\n",
        "        embedded = self.dropout(self.embedding(input).float())\n",
        "\n",
        "        # embedded = [1, batch size, emb dim]\n",
        "        emb_con = torch.cat((embedded, context), dim=2)\n",
        "\n",
        "        # emb_con = [1, batch size, emb dim + hid dim]\n",
        "        output, hidden = self.rnn(emb_con, hidden)\n",
        "\n",
        "        # output = [seq len, batch size, hid dim * n directions]\n",
        "        # hidden = [n layers * n directions, batch size, hid dim]\n",
        "\n",
        "        # seq len, n layers and n directions will always be 1 in the decoder, therefore:\n",
        "        # output = [1, batch size, hid dim]\n",
        "        # hidden = [1, batch size, hid dim]\n",
        "\n",
        "        output = torch.cat(\n",
        "            (embedded.squeeze(0), hidden.squeeze(0), context.squeeze(0)), dim=1\n",
        "        )\n",
        "\n",
        "        # output = [batch size, emb dim + hid dim * 2]\n",
        "\n",
        "        prediction = self.softmax(self.fc_out(output))\n",
        "\n",
        "        # prediction = [batch size, output dim]\n",
        "\n",
        "        return prediction, hidden"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9fCOYTOKBW9v"
      },
      "outputs": [],
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "\n",
        "        assert (\n",
        "            encoder.hid_dim == decoder.hid_dim\n",
        "        ), \"Hidden dimensions of encoder and decoder must be equal!\"\n",
        "\n",
        "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
        "        # src = [src len, batch size]\n",
        "        # trg = [trg len, batch size]\n",
        "        # teacher_forcing_ratio is probability to use teacher forcing\n",
        "        # e.g. if teacher_forcing_ratio is 0.75 we use ground-truth inputs 75% of the time\n",
        "\n",
        "        batch_size = trg.shape[1]\n",
        "        trg_len = trg.shape[0]\n",
        "        trg_vocab_size = self.decoder.output_dim\n",
        "\n",
        "        # tensor to store decoder outputs\n",
        "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
        "\n",
        "        # last hidden state of the encoder is the context\n",
        "        context = self.encoder(src)\n",
        "\n",
        "        # context also used as the initial hidden state of the decoder\n",
        "        hidden = context\n",
        "\n",
        "        # first input to the decoder is the <sos> tokens\n",
        "        input = trg[0, :]\n",
        "\n",
        "        for t in range(1, trg_len):\n",
        "            # insert input token embedding, previous hidden state and the context state\n",
        "            # receive output tensor (predictions) and new hidden state\n",
        "            output, hidden = self.decoder(input, hidden, context)\n",
        "\n",
        "            # place predictions in a tensor holding predictions for each token\n",
        "            outputs[t] = output\n",
        "\n",
        "            # decide if we are going to use teacher forcing or not\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "\n",
        "            # get the highest predicted token from our predictions\n",
        "            top1 = output.argmax(1)\n",
        "\n",
        "            # if teacher forcing, use actual next token as next input\n",
        "            # if not, use predicted token\n",
        "            input = trg[t] if teacher_force else top1\n",
        "\n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WfDp_4o7BXh2",
        "outputId": "34405a40-b14d-422b-9f36-40faae239540"
      },
      "outputs": [],
      "source": [
        "# attn = Attention(HIDDEN_SIZE, HIDDEN_SIZE)\n",
        "\n",
        "encoder1 = Encoder(\n",
        "    VOCAB_SIZE, EMB_SIZE, HIDDEN_SIZE, 0, embedding_weights=np.array(embeddings)\n",
        ")\n",
        "decoder1 = Decoder(\n",
        "    VOCAB_SIZE, EMB_SIZE, HIDDEN_SIZE, 0.1, embedding_weights=np.array(embeddings)\n",
        ")\n",
        "\n",
        "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
        "\n",
        "# optimizer = torch.optim.Adam(encoder1.parameters(), lr = learning_rate , betas=(0.9, 0.98), eps=1e-9)\n",
        "# decoder_optimizer = torch.optim.Adam(encoder1.parameters(), lr = learning_rate, betas=(0.9, 0.98), eps=1e-9)\n",
        "\n",
        "model = Seq2Seq(encoder1, decoder1, DEVICE).to(DEVICE)\n",
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "print(f\"The model has {count_parameters(model):,} trainable parameters\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "current_time = lambda: time.strftime(\"%Y-%m-%d-%H:%M:%S\", time.localtime())\n",
        "print(current_time())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 450
        },
        "id": "K9_i-sP-TPDO",
        "outputId": "1bfc18f8-e0c5-46b6-f056-0689c0eade11"
      },
      "outputs": [],
      "source": [
        "NUM_EPOCHS = 1\n",
        "CLIP = 1 # gradient clipping\n",
        "RESUME = False\n",
        "\n",
        "train_iter = Hdf5Dataset(\n",
        "    pl.Path(folder) / train_filename, num_entries=N_SAMPLES)\n",
        "train_dataloader = DataLoader(\n",
        "    train_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
        "val_iter = Hdf5Dataset(pl.Path(folder) / valid_filename, num_entries=N_SAMPLES)\n",
        "val_dataloader = DataLoader(\n",
        "    val_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
        "\n",
        "# make sure folder exists\n",
        "pl.Path(\"checkpoints\").mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "model.train()\n",
        "if RESUME:\n",
        "    checkpoint = torch.load(\n",
        "        pl.Path(\"checkpoints\") /\n",
        "        f\"model-epoch_{NUM_EPOCHS-1}-{current_time()}.pt\"\n",
        "    )\n",
        "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "    optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
        "    epoch = checkpoint[\"epoch\"]\n",
        "\n",
        "for epoch in range(1, NUM_EPOCHS + 1):\n",
        "    start_time = timer()\n",
        "    print(\n",
        "        f\"\\033[92mEpoch {epoch} of {NUM_EPOCHS} - time: {current_time()}\\033[0m\")\n",
        "    print(f\"\\033[92mTraining...\\033[0m\")\n",
        "    train_loss = train(model, train_dataloader, optimizer, loss_fn, 0)\n",
        "    end_time = timer()\n",
        "    print(f\"\\033[92mValidating...\\033[0m\")\n",
        "    val_loss = evaluate(model, val_dataloader, loss_fn)\n",
        "    print(\n",
        "        (\n",
        "            f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"\n",
        "            f\"Epoch time = {(end_time - start_time):.3f}s\"\n",
        "        )\n",
        "    )\n",
        "    torch.save(\n",
        "        {\n",
        "            \"epoch\": epoch,\n",
        "            \"model_state_dict\": model.state_dict(),\n",
        "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "            \"loss\": val_loss,\n",
        "        },\n",
        "        pl.Path(\"checkpoints\") /\n",
        "        f\"model-epoch_{NUM_EPOCHS-1}-{current_time()}.pt\",\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0TE0FdC5VOBx"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "# function to generate output sequence using greedy algorithm\n",
        "def correct_sentence_vectorized(src_tensor, model, max_len=50):\n",
        "    assert isinstance(src_tensor, torch.Tensor)\n",
        "\n",
        "    model.eval()\n",
        "    src_tensor = src_tensor.unsqueeze(1).to(DEVICE)\n",
        "    # get length of input sentence\n",
        "    src_len = src_tensor.shape[0]\n",
        "\n",
        "    trg_vocab_size = model.decoder.output_dim\n",
        "\n",
        "    # tensor to store decoder outputs\n",
        "    outputs = torch.zeros(max_len, 1, trg_vocab_size).to(DEVICE)\n",
        "\n",
        "    # last hidden state of the encoder is the context\n",
        "    with torch.no_grad():\n",
        "        context = model.encoder(src_tensor)\n",
        "\n",
        "    # context also used as the initial hidden state of the decoder\n",
        "    hidden = context\n",
        "\n",
        "    # first input to the decoder is the <sos> tokens\n",
        "    input = src_tensor[0, :]\n",
        "    # enc_src = [batch_sz, src_len, hid_dim]\n",
        "    # Even though some examples might have been completed by producing a <eos> token\n",
        "    # we still need to feed them through the model because other are not yet finished\n",
        "    # and all examples act as a batch. Once every single sentence prediction encounters\n",
        "    # <eos> token, then we can stop predicting.\n",
        "    for t in range(1, max_len):\n",
        "        # insert input token embedding, previous hidden state and the context state\n",
        "        # receive output tensor (predictions) and new hidden state\n",
        "        output, hidden = model.decoder(input, hidden, context)\n",
        "\n",
        "        # place predictions in a tensor holding predictions for each token\n",
        "        outputs[t] = output\n",
        "\n",
        "        # get the highest predicted token from our predictions\n",
        "        top1 = output.argmax(1)\n",
        "\n",
        "        # if teacher forcing, use actual next token as next input\n",
        "        # if not, use predicted token\n",
        "        input = top1\n",
        "\n",
        "    pred_sentence = []\n",
        "\n",
        "    for i in range(1, len(outputs)):\n",
        "        topv, topi = outputs[i, :, :].topk(1)\n",
        "        pred_sentence.append(vocab_transform.vocab.itos_[topi])\n",
        "        if topi == EOS_IDX:\n",
        "            break\n",
        "\n",
        "    return \" \".join(pred_sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3dDyDJ4UVeL4",
        "outputId": "dd58acf2-4979-4799-8072-fe06c6981d4a"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "latest_checkpoint = sorted(Path(\"checkpoints\").glob(\"*.pt\"), key=os.path.getmtime)[-1]\n",
        "\n",
        "checkpoint = torch.load(latest_checkpoint)\n",
        "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "\n",
        "model.eval()\n",
        "\n",
        "# Pick one in 18M examples\n",
        "val_iter = Hdf5Dataset(pl.Path(folder) / valid_filename, num_entries=None)\n",
        "\n",
        "src, trg = random.choice(val_iter)\n",
        "\n",
        "print('input: \"', src, '\"')\n",
        "print('target: \"', trg, '\"')\n",
        "\n",
        "src = text_transform(src)\n",
        "\n",
        "print(f\"\\033[91mModel's prediction: \\033[0m\", end=\"\")\n",
        "print(correct_sentence_vectorized(src, model))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
